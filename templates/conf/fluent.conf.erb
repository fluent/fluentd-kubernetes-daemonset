<% target = (dockerfile.split("/").last.split("-").last) %>
<% is_alpine = (dockerfile.split("/").last.split("-").first == "alpine") %>

# AUTOMATICALLY GENERATED
# DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/fluent.conf.erb

<% if !is_alpine %>
@include systemd.conf
<% end%>
@include kubernetes.conf

<% case target when "elasticsearch"%>
<match **>
   type elasticsearch
   log_level info
   include_tag_key true
   host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
   port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
   scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
   ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
   user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
   password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
   reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'true'}"
   logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
   logstash_format true
   buffer_chunk_limit 2M
   buffer_queue_limit 32
   flush_interval 5s
   max_retry_wait 30
   disable_retry_limit
   num_threads 8
</match>
<%  when "logentries"%>
<match **>
   type logentries
   use_json true
   tag_access_log stdout
   tag_error_log stderr
   config_path /etc/logentries/tokens.yaml
</match>
<%  when "loggly"%>
<match **>
   type loggly
   log_level info
   loggly_url "https://logs-01.loggly.com/inputs/#{ENV['LOGGLY_TOKEN']}/tag/#{ENV['LOGGLY_TAGS'] || 'fluentd'}"
</match>
<%  when "cloudwatch"%>
<match **>
  type cloudwatch_logs
  log_group_name "#{ENV['LOG_GROUP_NAME']}"
  auto_create_stream true
  use_tag_as_stream true
</match>
<%  when "s3"%>
<match **>
  # docs: https://docs.fluentd.org/v0.12/articles/out_s3
  # note: this configuration relies on the nodes have an IAM instance profile with access to your S3 bucket
  type s3
  log_level info
  s3_bucket "#{ENV['S3_BUCKET_NAME']}"
  s3_region "#{ENV['S3_BUCKET_REGION']}"
  s3_object_key_format %{path}%{time_slice}/cluster-log-%{index}.%{file_extension}
  time_slice_format %Y/%m/%d
  time_slice_wait 10m
  utc
  include_time_key true
  include_tag_key true
  buffer_chunk_limit 256m
  buffer_path /var/log/fluentd-buffers/s3.buffer
</match>
<%  when "gcs"%>
<match **>
  # docs: https://github.com/daichirata/fluent-plugin-gcs
  # this configuration relies on the nodes having permission to write on your gs bucket
  type gcs
  project "#{ENV['GCS_BUCKET_PROJECT']}"
  bucket "#{ENV['GCS_BUCKET_NAME']}"
  object_key_format %{path}%{time_slice}/%{hostname}/%{index}.%{file_extension}
  path logs/
  buffer_path /var/log/fluentd-buffers/gcs.buffer
  time_slice_format %Y/%m/%d
  time_slice_wait 10m
  utc
</match>
<% when "stackdriver"%>
<match **>
  type google_cloud

  # Set the buffer type to file to improve the reliability and reduce the memory consumption
  buffer_type file
  buffer_path /var/log/fluentd-buffers/stackdriver.buffer
  # Set queue_full action to block because we want to pause gracefully
  # in case of the off-the-limits load instead of throwing an exception
  buffer_queue_full_action block
  # Set the chunk limit conservatively to avoid exceeding the GCL limit
  # of 10MiB per write request.
  buffer_chunk_limit 2M
  # Cap the combined memory usage of this buffer and the one below to
  # 2MiB/chunk * (6 + 2) chunks = 16 MiB
  buffer_queue_limit 6
  # Never wait more than 5 seconds before flushing logs in the non-error case.
  flush_interval 5s
  # Never wait longer than 30 seconds between retries.
  max_retry_wait 30
  # Disable the limit on the number of retries (retry forever).
  # disable_retry_limit
  # Use multiple threads for processing.
  num_threads 2
</match>
<% when "graylog"%>
<match **>
   type gelf
   log_level info
   include_tag_key true
   host "#{ENV['FLUENT_GRAYLOG_HOST']}"
   port "#{ENV['FLUENT_GRAYLOG_PORT']}"
   buffer_chunk_limit 4096K
   buffer_queue_limit 512
   flush_interval 5s
   max_retry_wait 30
   disable_retry_limit
   num_threads 8
<% when "logzio"%>
<match **>
  type logzio_buffered
  endpoint_url "https://listener.logz.io:8071?token=#{ENV['LOGZIO_TOKEN']}&type=#{ENV['LOGZIO_LOGTYPE']}"
  output_include_time true
  output_include_tags true

  # Set the buffer type to file to improve the reliability and reduce the memory consumption
  buffer_type file
  buffer_path /var/log/fluentd-buffers/stackdriver.buffer
  # Set queue_full action to block because we want to pause gracefully
  # in case of the off-the-limits load instead of throwing an exception
  buffer_queue_full_action block
  # Set the chunk limit conservatively to avoid exceeding the GCL limit
  # of 10MiB per write request.
  buffer_chunk_limit 2M
  # Cap the combined memory usage of this buffer and the one below to
  # 2MiB/chunk * (6 + 2) chunks = 16 MiB
  buffer_queue_limit 6
  # Never wait more than 5 seconds before flushing logs in the non-error case.
  flush_interval 5s
  # Never wait longer than 30 seconds between retries.
  max_retry_wait 30
  # Disable the limit on the number of retries (retry forever).
  # disable_retry_limit
  # Use multiple threads for processing.
  num_threads 2
</match>

<% when "papertrail"%>

## Capture audit logs
#<match kube-apiserver-audit>
#  type papertrail
#
#  papertrail_host "#{ENV['FLUENT_PAPERTRAIL_AUDIT_HOST']}"
#  papertrail_port "#{ENV['FLUENT_PAPERTRAIL_AUDIT_PORT']}"
#</match>

<match **>
  type papertrail

  papertrail_host "#{ENV['FLUENT_PAPERTRAIL_HOST']}"
  papertrail_port "#{ENV['FLUENT_PAPERTRAIL_PORT']}"

</match>
<% when "kafka" %>
<filter **>
  @type prometheus
  <metric>
    name fluentd_input_status_num_records_total
    type counter
    desc The total number of incoming records
    <labels>
      tag ${tag}
      hostname ${hostname}
    </labels>
  </metric>
</filter>
<match **>
  @type copy
  <store>
    @type prometheus
    <metric>
      name fluentd_output_status_num_records_total
      type counter
      desc The total number of outgoing records
      <labels>
        tag ${tag}
        hostname ${hostname}
      </labels>
    </metric>
  </store>
</match>
<source>
  @type prometheus
  bind 0.0.0.0
  port 24231
  metrics_path /metrics
</source>
<source>
  @type prometheus_output_monitor
  interval 10
  <labels>
    hostname ${hostname}
  </labels>
</source>
<match **>
  type kafka_buffered

  brokers "#{ENV['FLUENT_KAFKA_BROKERS']}"

  default_topic "#{ENV['FLUENT_KAFKA_DEFAULT_TOPIC'] || nil}"
  default_partition_key "#{ENV['FLUENT_KAFKA_DEFAULT_PARTITION_KEY'] || nil}"
  default_message_key "#{ENV['FLUENT_KAFKA_DEFAULT_MESSAGE_KEY'] || nil}"
  output_data_type "#{ENV['FLUENT_KAFKA_OUTPUT_DATA_TYPE'] || 'json'}"
  output_include_tag "#{ENV['FLUENT_KAFKA_OUTPUT_INCLUDE_TAG'] || false}"
  output_include_time "#{ENV['FLUENT_KAFKA_OUTPUT_INCLUDE_TIME'] || false}"
  exclude_topic_key "#{ENV['FLUENT_KAFKA_EXCLUDE_TOPIC_KEY'] || false}"
  exclude_partition_key "#{ENV['FLUENT_KAFKA_EXCLUDE_PARTITION_KEY'] || false}"
  get_kafka_client_log "#{ENV['FLUENT_KAFKA_GET_KAFKA_CLIENT_LOG'] || false}"

  # ruby-kafka producer options
  max_send_retries "#{ENV['FLUENT_KAFKA_MAX_SEND_RETRIES'] || 1}"
  required_acks "#{ENV['FLUENT_KAFKA_REQUIRED_ACKS'] || -1}"
  ack_timeout "#{ENV['FLUENT_KAFKA_ACK_TIMEOUT'] || nil}"
  compression_codec "#{ENV['FLUENT_KAFKA_COMPRESSION_CODEC'] || nil}"
  max_send_limit_bytes "#{ENV['FLUENT_KAFKA_MAX_SEND_LIMIT_BYTES'] || nil}"
  discard_kafka_delivery_failed "#{ENV['FLUENT_KAFKA_DISCARD_KAFKA_DELIVERY_FAILED'] || false}"
</match>
  
<% when "syslog" %>
<match **>
  @type kubernetes_remote_syslog
  host "#{ENV['SYSLOG_HOST']}"
  port "#{ENV['SYSLOG_PORT']}"
  severity debug
  tag fluentd
  protocol tcp
  packet_size 65535
  output_data_type ltsv
</match>
<% end%>
